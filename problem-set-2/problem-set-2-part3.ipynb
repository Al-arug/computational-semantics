{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2 - Part 3 (VG): Ambiguity and Inference\n",
    "\n",
    "This lab should be completed individually.\n",
    "\n",
    "It extend the topics in the previous two parts with additional questions.\n",
    "\n",
    "This assignment has 27 marks in total.\n",
    "\n",
    "### Pre-requisite knowledge\n",
    "\n",
    "From `problem-set-1`:\n",
    "- First order logic\n",
    "- Lambda calculus\n",
    "- Feature unification context free grammar\n",
    "\n",
    "From `problem-set-2-part1.ipynb`:\n",
    "- Cooper storage\n",
    "\n",
    "From `problem-set-2-part2.ipynb`:\n",
    "- Using Prover9\n",
    "- Model building with `mace4`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This task needs NLTK and Jupyter Notebook (IPython package).\n",
    "\n",
    "import nltk\n",
    "from nltk.grammar import FeatureGrammar\n",
    "from nltk.sem import cooper_storage as cs\n",
    "\n",
    "from utils import display_latex, display_translation, display_tree, display, Markdown\n",
    "from itertools import zip_longest\n",
    "\n",
    "read_expr = nltk.sem.Expression.fromstring\n",
    "\n",
    "from utils2 import sem_parser, evaluate_sentences, syntax, syntax_notv, compare_synsem\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types in Cooper Storage [12 marks in total]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Types of syntactic categories [5 marks in total]\n",
    "\n",
    "In this question we are going to look at alternative representations for transitive verbs and verb phrases from what you saw in `problem-set-2-part1`. \n",
    "If you look carefully you see that the `TV`, `DTV` categories have simpler representations.\n",
    "That means that `VP`s have to have more complex representations as shown below.\n",
    "If you run the code below you will see that it produces the same readings that you saw earlier with a different grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\"0. all dogs bite a bone\": $\\forall\\ x.(dog(x)\\ \\rightarrow\\ \\exists\\ z_{1}.(bone(z_{1})\\ \\land\\ bite(x,z_{1})))$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\"1. a man gives a bone to every dog\": $\\exists\\ x.(man(x)\\ \\land\\ \\exists\\ z_{7}.(bone(z_{7})\\ \\land\\ \\forall\\ z_{6}.(dog(z_{6})\\ \\rightarrow\\ give(x,z_{7},z_{6}))))$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\"1. a man gives a bone to every dog\": $\\exists\\ x.(man(x)\\ \\land\\ \\forall\\ z_{4}.(dog(z_{4})\\ \\rightarrow\\ \\exists\\ z_{3}.(bone(z_{3})\\ \\land\\ give(x,z_{3},z_{4}))))$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\"2. Russia gives Moscow to Napoleon\": $give(russia,moscow,napoleon)$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fcfg_string = r\"\"\"\n",
    "VP[NUM=?n,SEM=<\\x.(?obj(?v(x)))>] -> TV[NUM=?n,SEM=?v] NP[SEM=?obj]\n",
    "VP[NUM=?n,SEM=<\\x.(?pp(\\y.?obj(?v(x, y))))>] -> DTV[NUM=?n,SEM=?v] NP[SEM=?obj] PP[+TO,SEM=?pp]\n",
    "VP[NUM=?n,SEM=<\\x.(?obj(\\z.?pp(\\y.?v(x, y, z))))>] -> DTV[NUM=?n,SEM=?v] NP[SEM=?obj] PP[+TO,SEM=?pp]\n",
    "\n",
    "TV[NUM=sg,SEM=<\\x y.bite(x,y)>,TNS=pres] -> 'bites'\n",
    "TV[NUM=pl,SEM=<\\x y.bite(x,y)>,TNS=pres] -> 'bite'\n",
    "DTV[NUM=sg,SEM=<\\x z y.give(x,y,z)>,TNS=pres] -> 'gives'\n",
    "DTV[NUM=pl,SEM=<\\x z y.give(x,y,z)>,TNS=pres] -> 'give'\n",
    "\"\"\"\n",
    "\n",
    "# this is going to add new rules to the syntax:\n",
    "new_syntax = FeatureGrammar(\n",
    "    syntax_notv.start(),\n",
    "    syntax_notv.productions() + FeatureGrammar.fromstring(fcfg_string).productions()\n",
    ")\n",
    "\n",
    "sentences = [\n",
    "    'all dogs bite a bone',\n",
    "    'a man gives a bone to every dog',\n",
    "    'Russia gives Moscow to Napoleon',\n",
    "]\n",
    "\n",
    "# change `verbose` to see the trees\n",
    "sents_reps = sem_parser(sentences, new_syntax, verbose=False)\n",
    "\n",
    "for i, (sent, semreps) in enumerate(sents_reps.items()):\n",
    "    for semrep in semreps:\n",
    "        display_translation(f\"{i}. {sent}\", semrep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this question is to inspect and compare two sets of grammar rules from `syntax`, `new_syntax`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| Category | Grammar 1 | Grammar 2 \n",
       "|----------|-----------|----------\n",
       "| DTV -> {{STR}} | \\Y X x.X(\\z.Y(\\y.give(x,y,z))) | \\x z y.give(x,y,z)\n",
       "| NP -> Det Nom | ?det(?nom) | ?det(?nom)\n",
       "| NP -> PropN | ?np | ?np\n",
       "| PP -> P NP | ?np | ?np\n",
       "| S -> NP VP | ?subj(?vp) | ?subj(?vp)\n",
       "| TV -> {{STR}} | \\X x.X(\\y.bite(x,y)) | \\x y.bite(x,y)\n",
       "| VP -> DTV NP PP | ?v(?obj,?pp) | \\x.?pp(\\y.?obj(?v(x,y)))\n",
       "| VP -> DTV NP PP | - | \\x.?obj(\\z.?pp(\\y.?v(x,y,z)))\n",
       "| VP -> IV | ?v | ?v\n",
       "| VP -> TV NP | ?v(?obj) | \\x.?obj(?v(x))\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display the comparison\n",
    "table = \"\"\"\n",
    "| Category | Grammar 1 | Grammar 2 \n",
    "|----------|-----------|----------\n",
    "\"\"\"\n",
    "for cat, sem1, sem2 in compare_synsem(syntax, new_syntax):\n",
    "    if cat[0] in ['VP', 'DTV', 'TV', 'NP', 'S', 'PP']:\n",
    "        table += f\"| {cat[0]} -> {cat[1]} | {'-' if sem1 is None else sem1} | {'-' if sem2 is None else sem2}\\n\"\n",
    "\n",
    "display(Markdown(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1a.** What are the semantic types of each syntactic category?\n",
    "Compare `new_syntax` with `syntax` imported form `util2.py`. \n",
    "\n",
    "Write types in `???`. **[4 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|\tSyntactic Type\t|\tGrammar 1\t\t\t|\tGrammar 2\t\t\t| \n",
    "|-------------------|-----------------------|-----------------------| \n",
    "| S\t\t\t\t\t| `t`\t\t\t\t\t| `t`\t\t\t\t\t|\n",
    "| DET\t\t\t\t| `<<e,t>,<<e,t>,t>>`\t| `<<e,t>,<<e,t>,t>>`\t|\n",
    "| N\t\t\t\t\t| `<e,t>`\t\t\t\t| `<e,t>`\t\t\t\t|\n",
    "| IV\t\t\t\t| `<e,t>`\t\t\t\t| `<e,t>`\t\t\t\t|\n",
    "| TV\t\t\t\t| `<<<e,t>,t>,<e,t>>`\t| `<e,t>`\t\t\t\t|\n",
    "| DTV\t\t\t\t| `<e,t>,t>`\t\t\t| `<e,t>`\t\t\t    |\n",
    "| NP\t\t\t\t| `<e,t>,t>`\t\t\t| `<e,t>,t>`\t\t\t|\n",
    "| PP\t\t\t\t| `<e,t> `\t\t\t    | `<<e,t>,t>`\t\t\t\t|\n",
    "| VP\t\t\t\t| `<e,t>`\t\t\t\t| `<e,t>`\t\t\t    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1b.** There are two simple ways to construct lambda terms in lambda calculus:\n",
    "\n",
    "|\tLambda Syntax\t|\tName\t\t\t\t|\tDescription\t\t\t|\n",
    "|-------------------|-----------------------|-----------------------|\n",
    "|\tP(X)\t\t\t|\tapplication (app)\t|\tapplying P on X.\t|\n",
    "|\t\\x.P\t\t\t|\tabstraction (abs)\t|\tfunction definition.|\n",
    "\n",
    "Compare the composition of non-terminal nodes as implemented in two grammars **[1 mark]**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|\tSyntactic Type\t\t|\tGrammar 1\t\t|\tGrammar 2\t\t| \n",
    "|-----------------------|-------------------|-------------------| \n",
    "|\t`S  -> NP VP`\t\t|\tapp\t\t\t\t|\tapp\t\t\t\t|\n",
    "|\t`VP -> IV`\t\t\t|\tnone\t\t\t|\tnone\t\t\t|\n",
    "|\t`NP -> DET Nom`\t\t|\t`app`\t\t\t|\t`app`\t\t\t|\n",
    "|\t`VP -> TV NP`\t\t|\t`app`\t\t\t|\tabs,app\t\t\t|\n",
    "|\t`VP -> DTV NP PP`\t|\t`app`\t\t\t|\t`abs,app`\t\t\t|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. A new conjunction rule [7 marks in total]\n",
    "\n",
    "The grammar below implements Cooper Storage - in fact this version is included as `storage.fcfg` with NLTK. In the lecture and in the previous part of this assignment we used our modified version. The goal of this question is to examine why we had to modify this version of the grammar. In this grammar the lexical representations of verbs are basic but their compositions with NPs are not complex as in Question 1 of this notebook. Therefore, this grammar also requires a different conjunction rule from the one in `problem-set-2-part1`.\n",
    "\n",
    "Run the code below and answer questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fcfg_storage = r\"\"\"\n",
    "%start S\n",
    "\n",
    "S[SEM=[CORE=<?vp(?subj)>, STORE=(?b1+?b2)]] -> NP[SEM=[CORE=?subj, STORE=?b1]] VP[SEM=[CORE=?vp, STORE=?b2]]\n",
    "\n",
    "VP[SEM=?v] -> IV[SEM=?v]\n",
    "VP[SEM=[CORE=<?v(?obj)>, STORE=(?b1+?b2)]] -> TV[SEM=[CORE=?v, STORE=?b1]] NP[SEM=[CORE=?obj, STORE=?b2]]\n",
    "VP[SEM=[CORE=<?v(?pp)(?obj)>, STORE=(?b1+?b2+?b3)]] -> DTV[SEM=[CORE=?v, STORE=?b1]] NP[SEM=[CORE=?obj, STORE=?b2]] PP[+TO, SEM=[CORE=?pp, STORE=?b3]]  \n",
    "\n",
    "NP[SEM=[CORE=<@x>, STORE=((<bo(?det(?nom),@x)>)+?b1+?b2)]] -> Det[SEM=[CORE=?det, STORE=?b1]] Nom[SEM=[CORE=?nom, STORE=?b2]]\n",
    "Nom[SEM=?n] -> N[SEM=?n]\n",
    "NP[SEM=?np] -> PropN[SEM=?np]\n",
    "\n",
    "PP[+TO, SEM=[CORE=?np, STORE=?b1]] -> P NP[SEM=[CORE=?np, STORE=?b1]]\n",
    "\n",
    "# Lexical items:\n",
    "Det[SEM=[CORE=<\\Q P.exists x.(Q(x) & P(x))>, STORE=(/)]] -> 'a'\n",
    "Det[SEM=[CORE=<\\Q P.all x.(Q(x) implies P(x))>, STORE=(/)]] -> 'every'\n",
    "\n",
    "N[SEM=[CORE=<\\x.library(x)>, STORE=(/)]] -> 'library' \n",
    "N[SEM=[CORE=<\\x.book(x)>, STORE=(/)]] -> 'book' \n",
    "N[SEM=[CORE=<\\x.girl(x)>, STORE=(/)]] -> 'girl'\n",
    "N[SEM=[CORE=<\\x.boy(x)>, STORE=(/)]] -> 'boy'\n",
    "\n",
    "IV[SEM=[CORE=<\\x.smile(x)>, STORE=(/)]] -> 'smiles' \n",
    "IV[SEM=[CORE=<\\x.walk(x)>, STORE=(/)]] -> 'walks'\n",
    "\n",
    "TV[SEM=[CORE=<\\y x.read(x,y)>, STORE=(/)]] -> 'reads' \n",
    "\n",
    "DTV[SEM=[CORE=<\\z y x.give(x,y,z)>, STORE=(/)]] -> 'gives' \n",
    "\n",
    "PropN[SEM=[CORE=<@x>, STORE=(<bo(\\P.P(angus),@x)>)]] -> 'Angus' \n",
    "PropN[SEM=[CORE=<@x>, STORE=(<bo(\\P.P(cyril),@x)>)]] -> 'Cyril'\n",
    "\n",
    "P[+TO] -> 'to'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare the construction of representations in `SEM.CORE` with the grammar in Question 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| Category | Grammar 2 | Grammar 3 - CORE | Grammar 3 - STORE\n",
       "|----------|-----------|------------------|-------------------\n",
       "| `DTV -> {{STR}}` | `\\x z y.give(x,y,z)` | `\\z y x.give(x,y,z)` | `-` \n",
       "| `DTV -> {{STR}}` | `\\x z y.give(x,y,z)` | `-` | `-` \n",
       "| `NP -> Det Nom` | `?det(?nom)` | `@x` | `@x --> ?det(?nom)` \n",
       "| `NP -> PropN` | `?np` | `?np` | `-` \n",
       "| `PP -> P NP` | `?np` | `?np` | `-` \n",
       "| `S -> NP VP` | `?subj(?vp)` | `?vp(?subj)` | `-` \n",
       "| `TV -> {{STR}}` | `\\x y.bite(x,y)` | `-` | `-` \n",
       "| `TV -> {{STR}}` | `\\x y.bite(x,y)` | `\\y x.read(x,y)` | `-` \n",
       "| `VP -> DTV NP PP` | `\\x.?obj(\\z.?pp(\\y.?v(x,y,z)))` | `-` | `-` \n",
       "| `VP -> DTV NP PP` | `\\x.?pp(\\y.?obj(?v(x,y)))` | `?v(?pp,?obj)` | `-` \n",
       "| `VP -> IV` | `?v` | `?v` | `-` \n",
       "| `VP -> TV NP` | `\\x.?obj(?v(x))` | `?v(?obj)` | `-` \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cs_syntax = FeatureGrammar.fromstring(fcfg_storage)\n",
    "\n",
    "# display the comparison\n",
    "table = \"\"\"\n",
    "| Category | Grammar 2 | Grammar 3 - CORE | Grammar 3 - STORE\n",
    "|----------|-----------|------------------|-------------------\n",
    "\"\"\"\n",
    "for cat, sem1, sem2 in compare_synsem(cs_syntax, new_syntax):\n",
    "    if cat[0] in ['VP', 'DTV', 'TV', 'NP', 'S', 'PP']:\n",
    "        if sem1 is not None and type(sem1)!=nltk.grammar.FeatStructNonterminal:\n",
    "             # then there is a semantic representation, but its not structured:\n",
    "            core = sem1\n",
    "            store = '-'\n",
    "        elif type(sem1)==nltk.grammar.FeatStructNonterminal:\n",
    "            # then the semantic representation is structured:\n",
    "            core = str(sem1['CORE'])\n",
    "            store = \"\"\n",
    "            # find the binding operations introduced here:\n",
    "            for bo in re.findall(r'\\(bo\\((.+)\\)\\)', str(sem1['STORE'])):\n",
    "                rep, ref = tuple(bo.split(','))\n",
    "                store += f\"{ref} --> {rep}\" \n",
    "                \n",
    "            # if there is no new binding then ignore the store:\n",
    "            if store == \"\":\n",
    "                store = '-'\n",
    "        else:\n",
    "            # then there is no semantic representation:\n",
    "            core = '-'\n",
    "            store = '-'\n",
    "            \n",
    "        if sem2 is None:\n",
    "            sem2 = '-'\n",
    "            \n",
    "        table += f\"| `{cat[0]} -> {cat[1]}` \" +\\\n",
    "                 f\"| `{sem2}` \" +\\\n",
    "                 f\"| `{core}` \" +\\\n",
    "                 f\"| `{store}` \" +\\\n",
    "                  \"\\n\"\n",
    "\n",
    "display(Markdown(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2a.** What are the types of representations in `SEM.CORE` for each syntactic category? **[2 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|\tSyntactic Type\t|\tGrammar 3 -\tCORE\t\n",
    "|-------------------|-----------------------\n",
    "| S\t\t\t\t\t| `t`\t\t\t\t\t\n",
    "| Det\t\t\t\t| `<e,t>`\t\t\t\t\t\n",
    "| N\t\t\t\t\t| `t`\t\t\t\t\t\n",
    "| IV\t\t\t\t| `<e,t>`\t\t\t\t\t\n",
    "| TV\t\t\t\t| `<<<e,t>,t>,<e,t>>`\t\t\t\t\t\n",
    "| DTV\t\t\t\t| `<<t,<e,t>>`\t\t\t\t\t\n",
    "| NP\t\t\t\t| `???`\t\t\t\t\t\n",
    "| PP\t\t\t\t| `???`\t\t\t\t\t\n",
    "| VP\t\t\t\t| `<e,t>`\t\t\t\t\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b.** Compare the composition of non-terminal nodes in Cooper Storage and the grammar from the previous question. Fill in `???`. **[1 mark]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|\tSyntactic Type\t\t|\tGrammar Basic-CS\t|\tGrammar 2\t\t| \n",
    "|-----------------------|-----------------------|-------------------| \n",
    "|\t`S  -> NP VP`\t\t|\t`???`\t\t\t\t|\tapp\t\t\t\t|\n",
    "|\t`VP -> IV`\t\t\t|\tnone\t\t\t\t|\tnone\t\t\t|\n",
    "|\t`NP -> DET Nom`\t\t|\t`???`\t\t\t\t|\t`???`\t\t\t|\n",
    "|\t`VP -> TV NP`\t\t|\t`???`\t\t\t\t|\tabs/app\t\t\t|\n",
    "|\t`VP -> DTV NP PP`\t|\t`???`\t\t\t\t|\t`???`\t\t\t|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2c.** What are other differences between two representations?. **[1 mark]**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "** Write answers here **\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2d.** Finish the rule where the conjunction is handled in the  `STORE` by replacing `xxx` with an appropriate lambda function and then uncomment the rule. **[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcfg_conj = r\"\"\"\n",
    "# Conjunction rule:\n",
    "CONJ -> 'and'\n",
    "#NP[SEM=[CORE=<@x>, STORE=((<bo(xxx, @x)>)+?b1+?b2)]] -> NP[SEM=[CORE=?n1, STORE=?b1]] CONJ NP[SEM=[CORE=?n2, STORE=?b2]]\n",
    "#NP[SEM=[CORE=<yyy>, STORE=(?b1+?b2)]] -> NP[SEM=[CORE=?n1, STORE=?b1]] CONJ NP[SEM=[CORE=?n2, STORE=?b2]]\n",
    "\"\"\"\n",
    "\n",
    "cs_syntax = FeatureGrammar.fromstring(fcfg_storage+fcfg_conj)\n",
    "\n",
    "sentences = [\n",
    "    'every library gives a book to a girl and a boy',\n",
    "]\n",
    "\n",
    "sents_reps = sem_parser(sentences, cs_syntax, verbose=False, is_cs=True)\n",
    "\n",
    "counter = 0\n",
    "for i, (sent, semreps) in enumerate(sents_reps.items()):\n",
    "    for semrep in semreps:\n",
    "        # check if it is valid\n",
    "        if str(semrep).find(\"@\") == -1:\n",
    "            counter+=1\n",
    "            display_translation(f\"{counter}\", semrep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b.** Is it possible to write a conjunction rule in `yyy` in the `CORE`? Why? **[1 marks]**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "** Write your explanation here **\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference [15 marks in total]\n",
    "\n",
    "### 3. Ambiguous syntax and semantics and inference [15 marks in total]\n",
    "\n",
    "Extend the grammar in `problem-set-2-part2` to cover two sentences below that are similar to FraCaS problem 024. \n",
    "\n",
    "- no delegate obtained interesting results from the survey\n",
    "- no delegate obtained results from the survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3a.** Extend the grammar with a lexical entry for `obtained` as a `TV` category where it is combined with only one object. **[6 marks]**  \n",
    "Hint: Add a rule that attaches the `PP` to a bare plural `NP` without a `DET`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b.** Also add another rule for `obtained` where this is a `DTV` category. **[3 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: \"'no', 'delegate', 'obtained', 'interesting', 'results', 'from', 'the', 'survey'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c85e005cfac3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# change `verbose` to see the trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0msents_reps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msem_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_syntax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemreps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents_reps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/computational-semantics/problem-set-2/utils2.py\u001b[0m in \u001b[0;36msem_parser\u001b[0;34m(sents, syntax, verbose, is_cs)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \"\"\"\n\u001b[1;32m     82\u001b[0m     \u001b[0msents_reps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpret_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyntax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"----\\n{sent}: {len(results)} result(s)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/nltk/sem/util.py\u001b[0m in \u001b[0;36minterpret_sents\u001b[0;34m(inputs, grammar, semkey, trace)\u001b[0m\n\u001b[1;32m     86\u001b[0m     return [\n\u001b[1;32m     87\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_semrep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msyn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msyntrees\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msyntrees\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparse_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrammar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     ]\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/nltk/sem/util.py\u001b[0m in \u001b[0;36mparse_sents\u001b[0;34m(inputs, grammar, trace)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# use a tokenizer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0msyntrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mparses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyntrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/nltk/parse/chart.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens, tree_class)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m         \u001b[0mchart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchart_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtree_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/nltk/parse/chart.py\u001b[0m in \u001b[0;36mchart_parse\u001b[0;34m(self, tokens, trace)\u001b[0m\n\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0mchart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chart_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m         \u001b[0mgrammar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/nltk/grammar.py\u001b[0m in \u001b[0;36mcheck_coverage\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    684\u001b[0m                 \u001b[0;34m\"Grammar does not cover some of the \"\u001b[0m \u001b[0;34m\"input words: %r.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"'no', 'delegate', 'obtained', 'interesting', 'results', 'from', 'the', 'survey'\"."
     ]
    }
   ],
   "source": [
    "# your answers\n",
    "fcfg_storage_answers_a = r\"\"\"\n",
    "## Cover lexicon:\n",
    "Det -> 'no'\n",
    "N -> 'delegate'\n",
    "N -> 'results' \n",
    "Adj -> 'interesting'\n",
    "TV -> 'obtained'\n",
    "PP -> 'from' 'the' 'survey'\n",
    "# add any missing rules\n",
    "# NP -> N PP\n",
    "\"\"\"\n",
    "\n",
    "fcfg_storage_answers_b = r\"\"\"\n",
    "## Cover lexicon:\n",
    "DTV -> 'obtained'\n",
    "# add any missing rules\n",
    "\"\"\"\n",
    "\n",
    "sentences = [\n",
    "    \"no delegate obtained interesting results from the survey\",\n",
    "    \"no delegate obtained results from the survey\",\n",
    "]\n",
    "\n",
    "# this is going to add new rules to the syntax:\n",
    "grammar = FeatureGrammar(\n",
    "    syntax.start(),\n",
    "    syntax.productions() + \\\n",
    "    FeatureGrammar.fromstring(fcfg_storage_answers_a).productions() + \\\n",
    "    FeatureGrammar.fromstring(fcfg_storage_answers_b).productions()\n",
    ")\n",
    "\n",
    "# change `verbose` to see the trees\n",
    "sents_reps = sem_parser(sentences, new_syntax, verbose=False)\n",
    "\n",
    "for i, (sent, semreps) in enumerate(sents_reps.items()):\n",
    "    for semrep in semreps:\n",
    "        display_translation(f\"{i}. {sent}\", semrep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3c.** Use Mace to show whether any of the readings of the first sentence entail any of the readings of the second sentence. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use apply_model_builder from problem-set-2-part2\n",
    "\n",
    "# There is something missing in this code.\n",
    "#apply_model_builder(...,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3d.** Use Prover9 to show which readings of the second sentence entail a reading of the first sentence. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use apply_theorem_prover from problem-set-2-part2\n",
    "\n",
    "# There is something missing in this code.\n",
    "#apply_theorem_prover(...,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3e.** Possibly there is a reading of the second sentence that does not entail the first sentence.\n",
    "Which is it? \n",
    "Explain why the entailment does not hold? Does this reading agree with our intuitions about the meaning of the sentence? **[1 + 2 + 1 = 4 marks]**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "** Your answer **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Inference using Neural Networks\n",
    "Adam Ek\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Before starting, please read the instructions on [how to work on group assignments](https://github.com/sdobnik/computational-semantics/blob/master/README.md).\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the VG part of problem set 3, we will work with neural networks for natural language inference. Our task is: given a premise sentence P and hypothesis H, what entailment relationship holds between them? Is H entailed by P, contradicted by P or neutral towards P?\n",
    "\n",
    "Given a sentence P, if H definitely describe something true given P then it is an **entailment**. If H describe something that's *maybe* true given P, it's **neutral**, and if H describe something that's definitely *false* given P it's a **contradiction**. \n",
    "\n",
    "This definition of inference, and the method we use to solve it, is diffrent from what you've previously worked with. Briefly discuss strengths and weaknesses of using formal semantics versus using statistical methods for natural language inference. **[4 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here**\n",
    "\n",
    "Formal semantics: It requires a lot of hand work for categorizing and creating logical operations. The accuracy of the results might by higher than statistical models in tasks as question answering as formal semantics could make language generalization that include syntax. However, given the complixity of its modeling, it would be a computationally expensive alternative. On the morphological and sentential level, words and sentences need to be parsed, written and structured correctly in order to be identified and processed, so this makes it more error prone.  \n",
    "\n",
    "Statistical methods: It is more automated, but has less generalisation or knowledge of language, such as syntax. However, it could have richer word representation, as meaning carying where we see it in VSM. This representation might be corpus based though. This means a better model would require a larger data to creat a more inclusive language scheme that covers suffiecient representations. Larger corpus with improved word representations such as deep contexualized representation would make it a good at handling polysimys and synonyms where the context plays a crucial role. However, larger corpus would also require annotation and different structures depending on the task that model is aimed to train on. So being corpus dependent has its advantages and disadvantages. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore natural language inference using neural networks on the SNLI dataset, described in [1]. The dataset can be downloaded [here](https://nlp.stanford.edu/projects/snli/). We prepared a \"simplified\" version, with only the relevant columns [here](https://gubox.box.com/s/idd9b9cfbks4dnhznps0gjgbnrzsvfs4).\n",
    "\n",
    "The (simplified) data is organized as follows (tab-separated values):\n",
    "* Column 1: Premise\n",
    "* Column 2: Hypothesis\n",
    "* Column 3: Relation\n",
    "\n",
    "Like in the previous lab, we'll use torchtext to build a dataloader. You can essentially do the same thing as you did in the last lab, but with our new dataset. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "device = torch.device('cuda:0')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dataloader(path):\n",
    "        # tokenizer\n",
    "    whitespacer = lambda x: x.split(' ')\n",
    "\n",
    "   \n",
    "    \n",
    "    premises = Field(tokenize = whitespacer ,lower = True, batch_first = True,pad_token = \"<pad>\", unk_token = \"<unk>\",fix_length=16)\n",
    "    hypothesis = Field(tokenize = whitespacer , lower = True, batch_first = True, pad_token = \"<pad>\", unk_token = \"<unk>\", fix_length=16 )\n",
    "    relation = Field(batch_first = True,sequential=False, is_target=True)\n",
    "    \n",
    "    # create tabular datasets  \n",
    "    train, test = TabularDataset.splits(path = path,\n",
    "                                        train = \"simple_snli_1.0_train.csv\",\n",
    "                                        test = \"simple_snli_1.0_test.csv\",\n",
    "                                        format = \"csv\",\n",
    "                                        fields = [(\"premises\", premises),\n",
    "                                                  (\"hypothesis\", hypothesis),\n",
    "                                                  (\"relation\", relation)],\n",
    "                                        skip_header = True,\n",
    "                                        csv_reader_params = {\"delimiter\": \"\\t\"})\n",
    "    \n",
    "\n",
    "    \n",
    "    premises.build_vocab(train.premises,test.premises,train.hypothesis,test.hypothesis)\n",
    "    hypothesis.vocab = premises.vocab\n",
    "    relation.build_vocab(train, test,min_freq=3)\n",
    "\n",
    "    \n",
    "    train_iter, test_iter = BucketIterator.splits((train, test), \n",
    "                                                  batch_size=12,\n",
    "                                                  sort_within_batch=False,\n",
    "                                                  shuffle=True,\n",
    "                                                  sort_key=lambda x: (len(x.premises),(len(x.hypothesis))),\n",
    "                                                  device=device)\n",
    "    return train_iter, test_iter, premises, relation \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we'll build the model for predicting the relationship between H and P.\n",
    "\n",
    "We will process each sentence using an LSTM. Then, we will construct some representation of the sentence. When we have a representation for H and P, we will combine them into one vector which we can use to predict the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a model described in [2], the BiLSTM with mean/max-pooling model. The procedure for the model is roughly:\n",
    "\n",
    "    1) Encode the Hypothesis and the Premise using a bidirectional LSTM\n",
    "    2) Perform max or mean pooling over the premise and hypothesis\n",
    "    3) Combine the premise and hypothesis into one representation\n",
    "    4) Predict the relationship "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a representation of a sentence\n",
    "\n",
    "Let's first consider step 2 where we perform max/mean pooling. There is a function in pytorch for this, but we'll implement it from scratch. \n",
    "\n",
    "Let's consider the general case, what we want to do for these methods is apply some function $f$ along dimension $i$, and we want to do this for all $i$'s. As an example we consider the matrix S with size ``(N, D)`` where N is the number of words and D the number of dimensions:\n",
    "\n",
    "$S = \\begin{bmatrix}\n",
    "    s_{11} & s_{12} & s_{13} & \\dots  & s_{1d} \\\\\n",
    "    s_{21} & s_{22} & s_{23} & \\dots  & s_{2d} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    s_{n1} & s_{n2} & s_{n3} & \\dots  & s_{nd}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "What we want to do is apply our function $f$ on each dimension, taking the input $s_{1d}, s_{2d}, ..., s_{nd}$ and generating the output $x_d$. \n",
    "\n",
    "You will implement both the max and mean pooling methods. When performing mean-pooling, $f$ will be the mean function and $x$ is the output, thus for each dimension $d$ we calculate:\n",
    "\n",
    "\\begin{equation}\n",
    "x_d = \\frac{1}{N}\\sum_{j=1}^N x_{jd}\n",
    "\\end{equation}\n",
    "\n",
    "When performing max-pooling we do the same thing, but let $f$ be the ``argmax`` function:\n",
    "\n",
    "\\begin{equation}\n",
    "    x_d = f(s_{1d}, s_{2d}, ..., s_{nd}) = argmax(s_{1d}, s_{2d}, ..., s_{nd})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Both of these operations reduce a batch of size ``(batch_size, num_words, dimensions)`` to ``(batch_size, 1, dimensions)`` meaning that we now have created a sentence representation based on the content of the words representations in the sentence (by applying some function $f$ along a dimension). \n",
    "\n",
    "Create a function that takes as input a tensor of size ``(batch_size, num_words, dimensions)`` then performs max or mean pooling and return it. [**6 Marks**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "def pooling(input_tensor):  \n",
    "  \n",
    "    values,indecies = torch.max(input_tensor, 1)\n",
    "    l = torch.unsqueeze(values, 1)\n",
    "    return l\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining sentence representations\n",
    "\n",
    "Next, we need to combine the premise and hypothesis into one representation. We will do this by concatenating four tensors (the final size of our tensor $X$ will be ``(batch_size, 1, 4d)`` where ``d`` is the number of dimensions that you use): \n",
    "\n",
    "$$X = [P; H; P \\cdot H; P-H]$$\n",
    "\n",
    "Here, what we do is concatenating P, H, P times H, and the absolute value of P minus H, then return the result.\n",
    "\n",
    "Implement the function. **[4 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_premise_and_hypothesis(hypothesis, premise):\n",
    "    \n",
    "    output=[]\n",
    "    for i,l in enumerate(hypothesis):\n",
    "        a = hypothesis[i] - premise[i] \n",
    "        b = hypothesis[i] * premise[i] \n",
    "        l = output.append(torch.cat((hypothesis[i], premise[i],b,a), 1))\n",
    "        \n",
    "    return torch.squeeze(torch.stack(output),1)\n",
    "\n",
    "                      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "\n",
    "Finally, we can build the model according to the procedure given previously by using the functions we defined above. Additionaly, in the model you should use *dropout*. For efficiency purposes, it's acceptable to only train the model with either max or mean pooling. \n",
    "\n",
    "Implement the model [**6 marks**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIModel(nn.Module):\n",
    "    def __init__(self, vocab_length, embedding_dim, hidden_dim, num_labels):\n",
    "        # your code goes here\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_length, embedding_dim) # vocab_size=len(CONTEXTS.vocab); embedding_dim = 50 (say)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True, num_layers = 2, dropout=0.1 )\n",
    "        self.nf = nn.Linear(hidden_dim * 8, hidden_dim)\n",
    "        \n",
    "       \n",
    "        \n",
    "    def forward(self, premise, hypothesis):\n",
    "        \n",
    "        prem = self.embeddings(premise)\n",
    "        hypo = self.embeddings(hypothesis)\n",
    "        \n",
    "    \n",
    "        prem_encode, (h_p, lc_p) = self.rnn(prem)\n",
    "        hypo_encode, (h_h, lc_h) = self.rnn(hypo)\n",
    "     \n",
    "        \n",
    "        max_prem = pooling(prem_encode)\n",
    "        max_hypo = pooling(hypo_encode)\n",
    "        \n",
    "                \n",
    "        x = combine_premise_and_hypothesis(max_prem,max_hypo)\n",
    "        \n",
    "        lin= self.nf(x)  \n",
    "        \n",
    "      \n",
    "        return F.relu(lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, implement the training and testing of the model. SNLI can take a very long time to train, so I suggest you only run it for one or two epochs. **[2 marks]** \n",
    "\n",
    "**Tip for efficiency:** *when developing your model, try training and testing the model on one batch (for each epoch) of data to make sure everything works! It's very annoying if you train for N epochs to find out that something went wrong when testing the model, or to find that something goes wrong when moving from epoch 0 to epoch 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train done\n",
      "test done\n",
      "Accuracy: 0.4774\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_iter, test_iter , Vocab, relation  = dataloader(\"SNLI-data\")\n",
    "\n",
    "num_of_batches = [(batch_idx , batch)for batch_idx , batch in enumerate(train_iter)][-1][0]\n",
    "\n",
    "embedding_size= 50\n",
    "hidden_size= 25\n",
    "epochs =  3      \n",
    "output_size= 3  \n",
    "\n",
    "\n",
    "model = SNLIModel(len(Vocab.vocab), embedding_size, hidden_size, output_size)\n",
    "model = model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = loss_function.to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr= 0.001)\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion,epochs):\n",
    "    \n",
    "        for e in range(epochs):    \n",
    "\n",
    "            model.train()  \n",
    "            for batch in iterator:\n",
    "                optimizer.zero_grad()\n",
    "                prem  = batch.premises\n",
    "                hypo = batch.hypothesis\n",
    "                predictions = model(prem,hypo)\n",
    "                tag = batch.relation\n",
    "                loss = criterion(predictions, tag) \n",
    "                loss.backward()       \n",
    "                optimizer.step() \n",
    "                \n",
    "                \n",
    "train(model, train_iter,optimizer, loss_function,epochs)\n",
    "                \n",
    "print(\"train done\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = []\n",
    "    label = []\n",
    "    for batch_idx, batch in enumerate(test_iter):\n",
    "        hyp  = batch.hypothesis\n",
    "        prem = batch.premises\n",
    "        answer = model(prem,hyp)\n",
    "        pred1 = torch.max(answer, 1)[1].view(batch.relation.size()).tolist()\n",
    "        relation = batch.relation.tolist()\n",
    "        pred += pred1\n",
    "        label += relation\n",
    "        \n",
    "        \n",
    "print(\"test done\")\n",
    "\n",
    "acc = accuracy_score(pred,label)\n",
    "print(\"Accuracy:\", acc)    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest a baseline that we can compare our model against **[2 marks]**\n",
    "\n",
    "I cannot think of what a good baseline would be. Perhaps the average accuracy for the entailment - not entailment labels could be a baseline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways (other than using a baseline) in which we can analyse the models performance **[6 marks]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here**\n",
    "\n",
    "The model's preformance can be measured by testing on more hypothesis based on the same premise. This means  the model is actually practical and could preform on real-world tasks. In a practical task the model should be able to handle multiple hypothesis/inferences by whether they are entailed by the premise or not. Other way to measure the preformance of a model is the validity of a hypothesis depndeing on multiple premises. Meaning that a one hypothesis can be true/entailed given different premises, such as which correct context/premise entails a hypothesis. Of course this might require other datasets, but in genreal sense, a resemblense of human logical thinking should be ideally able to handle such reasoning problems. For example, a random question that human would ask is \"What do we understand from this text?\"\" what can we infere from it\", and how many inferences can be taken from it. A summary of a text consist of a multiple inferences/hypothesis where the total summary can be valid only if all the inferences are correct given the original text/premises.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways to improve the model **[4 marks]**.\n",
    "\n",
    "1. Including contextually richer embeddings/representations such as ELMO should hypothetically add stronger meaning to the sentence.  \n",
    "\n",
    "2. Hyperparameters to test the best possible paramaters that fit the task should add a reasonable improvement. \n",
    "\n",
    "3. Testing/improving different methods for computing the premise-hypothesise combination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readings\n",
    "\n",
    "[1] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). \n",
    "\n",
    "[2] Conneau, A., Kiela, D., Schwenk, H., Barrault, L., & Bordes, A. (2017). Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

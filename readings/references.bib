

@article{Turney:2010aa,
  author =        {Turney, Peter D and Pantel, Patrick and others},
  journal =       {Journal of artificial intelligence research},
  number =        {1},
  pages =         {141--188},
  title =         {From frequency to meaning: Vector space models of
                   semantics},
  volume =        {37},
  year =          {2010},
  doi =           {http://dx.doi.org/10.1613/jair.2934},
}

@article{Talman:2019aa,
  author =        {Talman, Aarne and Yli-Jyr{\"a}, Anssi and
                   Tiedemann, J{\"o}rg},
  journal =       {Natural Language Engineering},
  number =        {4},
  pages =         {467--482},
  publisher =     {Cambridge University Press},
  title =         {Sentence embeddings in NLI with iterative refinement
                   encoders},
  volume =        {25},
  year =          {2019},
  doi =           {10.1017/S1351324919000202},
  url =           {https://doi.org/10.1017/S1351324919000202},
}

@techreport{Pulman:2005ab,
  address =       {Oxford, United Kingdom},
  author =        {Pulman, Stephen G.},
  institution =   {Department of Computer Science, University of Oxford},
  type =          {lecture notes},
  title =         {Higher Order Logic in Semantics},
  year =          {2005},
}

@inproceedings{Peters:2018aa,
  address =       {New Orleans, Louisiana},
  author =        {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and
                   Gardner, Matt and Clark, Christopher and Lee, Kenton and
                   Zettlemoyer, Luke},
  booktitle =     {Proceedings of the 2018 Conference of the North
                   American Chapter of the Association for Computational
                   Linguistics: Human Language Technologies, Volume 1
                   (Long Papers)},
  month =         jun,
  pages =         {2227--2237},
  publisher =     {Association for Computational Linguistics},
  title =         {Deep Contextualized Word Representations},
  year =          {2018},
  doi =           {10.18653/v1/N18-1202},
  url =           {https://www.aclweb.org/anthology/N18-1202},
}

@inproceedings{Pennington:2014aa,
  author =        {Pennington, Jeffrey and Socher, Richard and
                   Manning, Christopher},
  booktitle =     {Proceedings of the 2014 conference on empirical
                   methods in natural language processing (EMNLP)},
  pages =         {1532--1543},
  title =         {Glove: Global vectors for word representation},
  year =          {2014},
  url =           {http://www.aclweb.org/anthology/D14-1162},
}

@techreport{Olah:2015aa,
  author =        {Olah, Christopher},
  institution =   {Google Brain},
  month =         {August 27},
  title =         {Understanding {LSTM}s},
  year =          {2015},
  url =           {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
}

@article{Mitchell:2010aa,
  author =        {Mitchell, Jeff and Lapata, Mirella},
  journal =       {Cognitive Science},
  number =        {8},
  pages =         {1388--1429},
  publisher =     {Blackwell Publishing Ltd},
  title =         {Composition in Distributional Models of Semantics},
  volume =        {34},
  year =          {2010},
  doi =           {10.1111/j.1551-6709.2010.01106.x},
  issn =          {1551-6709},
  url =           {http://dx.doi.org/10.1111/j.1551-6709.2010.01106.x},
}

@inproceedings{Mitchell:2008uq,
  address =       {Columbus, Ohio},
  author =        {Mitchell, Jeff and Lapata, Mirella},
  booktitle =     {Proceedings of ACL-08: HLT},
  pages =         {236--244},
  title =         {Vector-based Models of Semantic Composition},
  year =          {2008},
  url =           {https://www.aclweb.org/anthology/P08-1028/},
}

@techreport{Manning:2017aa,
  address =       {Simons Institute, Berkeley},
  author =        {Christopher Manning},
  institution =   {Stanford University},
  month =         {March 27th},
  type =          {talk},
  title =         {Representations for Language: From Word Embeddings to
                   Sentence Meanings},
  year =          {2017},
  url =           {https://simons.berkeley.edu/talks/christopher-manning-2017-
                  3-27},
}

@unpublished{Manning:2005aa,
  author =        {Christopher D. Manning},
  note =          {Lecture notes for CS224N/Ling 280},
  title =         {An Introduction to Formal Computational Semantics},
  year =          {2005},
  url =           {https://prod-c2g.s3.amazonaws.com/cs224n/Fall2012/files/cl-
                  semantics-new.pdf},
}

@inproceedings{Kageback:2016aa,
  author =        {K{\aa}geb{\"a}ck, Mikael and Salomonsson, Hans},
  booktitle =     {5th Workshop on Cognitive Aspects of the Lexicon
                   (CogALex)},
  organization =  {Association for Computational Linguistics},
  title =         {Word Sense Disambiguation using a Bidirectional LSTM},
  year =          {2016},
  abstract =      {In this paper we present a clean, yet effective,
                   model for word sense disambiguation. Our approach
                   leverage a bidirectional long short-term memory
                   network which is shared between all words. This
                   enables the model to share statistical strength and
                   to scale well with vocabulary size. The model is
                   trained end-to-end, directly from the raw text to
                   sense labels, and makes effective use of word order.
                   We evaluate our approach on two standard datasets,
                   using identical hyperparameter settings, which are in
                   turn tuned on a third set of held out data. We employ
                   no external resources (e.g. knowledge graphs,
                   part-of-speech tagging, etc), language specific
                   features, or hand crafted rules, but still achieve
                   statistically equivalent results to the best
                   state-of-the-art systems, that employ no such
                   limitations.},
  url =           {http://www.cse.chalmers.se/~kageback/word-sense-
                  disambiguation-using-a-bidirectional-lstm/},
}

@techreport{Jurafsky:2019aa,
  author =        {Jurafsky, Dan and Martin, James H.},
  institution =   {Stanford University and University of Colorado at
                   Boulder},
  month =         {October 16},
  type =          {Third edition draft},
  title =         {Speech and language processing: an introduction to
                   natural language processing, computational
                   linguistics, and speech recognition},
  year =          {2019},
  url =           {https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf},
}

@article{Goodman:2016aa,
  author =        {Goodman, Noah D. and Frank, Michael C.},
  journal =       {Trends in Cognitive Sciences},
  number =        {11},
  pages =         {818--829},
  publisher =     {Elsevier},
  title =         {Pragmatic Language Interpretation as Probabilistic
                   Inference},
  volume =        {20},
  year =          {2016},
  abstract =      {Understanding language requires more than the use of
                   fixed conventions and more than decoding
                   combinatorial structure. Instead, comprehenders make
                   exquisitely sensitive inferences about what
                   utterances mean given their knowledge of the speaker,
                   language, and context. Building on developments in
                   game theory and probabilistic modeling, we describe
                   the rational speech act (RSA) framework for pragmatic
                   reasoning. RSA models provide a principled way to
                   formalize inferences about meaning in context; they
                   have been used to make successful quantitative
                   predictions about human behavior in a variety of
                   different tasks and situations, and they explain why
                   complex phenomena, such as hyperbole and vagueness,
                   occur. More generally, they provide a computational
                   framework for integrating linguistic structure, world
                   knowledge, and context in pragmatic language
                   understanding.},
  annote =        {doi: 10.1016/j.tics.2016.08.005},
  doi =           {10.1016/j.tics.2016.08.005},
  isbn =          {1364-6613},
  url =           {https://doi.org/10.1016/j.tics.2016.08.005},
}

@article{Erk:2012aa,
  author =        {Erk, Katrin},
  journal =       {Language and Linguistics Compass},
  number =        {10},
  pages =         {635--653},
  publisher =     {Blackwell Publishing Ltd},
  title =         {Vector Space Models of Word Meaning and Phrase
                   Meaning: A Survey},
  volume =        {6},
  year =          {2012},
  abstract =      {Distributional models represent a word through the
                   contexts in which it has been observed. They can be
                   used to predict similarity in meaning, based on the
                   distributional hypothesis, which states that two
                   words that occur in similar contexts tend to have
                   similar meanings. Distributional approaches are often
                   implemented in vector space models. They represent a
                   word as a point in high-dimensional space, where each
                   dimension stands for a context item, and a word's
                   coordinates represent its context counts. Occurrence
                   in similar contexts then means proximity in space. In
                   this survey we look at the use of vector space models
                   to describe the meaning of words and phrases: the
                   phenomena that vector space models address, and the
                   techniques that they use to do so. Many word meaning
                   phenomena can be described in terms of semantic
                   similarity: synonymy, priming, categorization, and
                   the typicality of a predicate's arguments. But vector
                   space models can do more than just predict semantic
                   similarity. They are a very flexible tool, because
                   they can make use of all of linear algebra, with all
                   its data structures and operations. The dimensions of
                   a vector space can stand for many things: context
                   words, or non-linguistic context like images, or
                   properties of a concept. And vector space models can
                   use matrices or higher-order arrays instead of
                   vectors for representing more complex relationships.
                   Polysemy is a tough problem for distributional
                   approaches, as a representation that is learned from
                   all of a word's contexts will conflate the different
                   senses of the word. It can be addressed, using either
                   clustering or vector combination techniques. Finally,
                   we look at vector space models for phrases, which are
                   usually constructed by combining word vectors. Vector
                   space models for phrases can predict phrase
                   similarity, and some argue that they can form the
                   basis for a general-purpose representation framework
                   for natural language semantics.},
  doi =           {10.1002/lnco.362},
  issn =          {1749-818X},
  url =           {http://dx.doi.org/10.1002/lnco.362},
}

@incollection{Clark:2015aa,
  author =        {Clark, Stephen},
  booktitle =     {Handbook of Contemporary Semantics --- second
                   edition},
  chapter =       {16},
  editor =        {Lappin, Shalom and Fox, Chris},
  pages =         {493--522},
  publisher =     {Wiley -- Blackwell},
  title =         {Vector Space Models of Lexical Meaning},
  year =          {2015},
}

@book{Chierchia:2000uq,
  address =       {Cambridge, Mass},
  author =        {Chierchia, Gennaro and McConnell-Ginet, Sally},
  edition =       {2},
  publisher =     {MIT Press},
  title =         {Meaning and grammar: an introduction to semantics},
  year =          {2000},
  isbn =          {0262032694},
}

@inproceedings{Bowman:2015ac,
  author =        {Bowman, Samuel R. and Angeli, Gabor and
                   Potts, Christopher and Manning, Christopher D.},
  booktitle =     {Proceedings of the 2015 Conference on Empirical
                   Methods in Natural Language Processing (EMNLP)},
  publisher =     {Association for Computational Linguistics},
  title =         {A large annotated corpus for learning natural
                   language inference},
  year =          {2015},
  url =           {https://www.aclweb.org/anthology/D15-1075/},
}

@book{BlackburnBos:2005,
  author =        {Blackburn, Patrick and Bos, Johan},
  publisher =     {CSLI Publications},
  title =         {Representation and inference for natural language.
                   {A} first course in computational semantics},
  year =          {2005},
  url =           {http://www.coli.uni-saarland.de/publikationen/softcopies/
                  Blackburn:1997:RIN.pdf},
}

@book{Bird:2009aa,
  author =        {Bird, Steven and Klein, Ewan and Loper, Edward},
  publisher =     {O'Reilly},
  title =         {Natural language processing with {P}ython},
  year =          {2009},
  abstract =      {This is an introduction to natural language
                   processing, which supports a variety of language
                   technologies, from predictive text and email
                   filtering to automatic summarization and translation},
  isbn =          {9780596516499},
  url =           {http://nltk.org/book/},
}

@article{Bengio:2003aa,
  author =        {Bengio, Yoshua and Ducharme, R{\'e}jean and
                   Vincent, Pascal and Janvin, Christian},
  journal =       {Journal of Machine Learning Research},
  number =        {6},
  pages =         {1137--1155},
  title =         {A Neural Probabilistic Language Model},
  volume =        {3},
  year =          {2003},
  abstract =      {A goal of statistical language modeling is to learn
                   the joint probability function of sequences of words
                   in a language. This is intrinsically difficult
                   because of the curse of dimensionality: a word
                   sequence on which the model will be tested is likely
                   to be different from all the word sequences seen
                   during training. Traditional but very successful
                   approaches based on n-grams obtain generalization by
                   concatenating very short overlapping sequences seen
                   in the training set. We propose to fight the curse of
                   dimensionality by learning a distributed
                   representation for words which allows each training
                   sentence to inform the model about an exponential
                   number of semantically neighboring sentences. The
                   model learns simultaneously (1) a distributed
                   representation for each word along with (2) the
                   probability function for word sequences, expressed in
                   terms of these representations. Generalization is
                   obtained because a sequence of words that has never
                   been seen before gets high probability if it is},
  issn =          {15324435},
  url =           {http://search.ebscohost.com.ezproxy.ub.gu.se/login.aspx?
                  direct=true&db=buh&AN=11468885&site=ehost-live},
}

@unpublished{Bender:2020aa,
  author =        {Emily M. Bender and Alexander Koller},
  month =         {January 26},
  note =          {OpenReview Preprint, anonymous preprint under review},
  title =         {Climbing towards NLU: On Meaning, Form, and
                   Understanding in the Age of Data},
  year =          {2020},
  abstract =      {The success of the large neural language models on
                   many NLP tasks is exciting. However, we find that
                   that these successes sometimes lead to hype in which
                   these models are being described as "understanding"
                   language or capturing "meaning". In this position
                   paper, we argue that a system trained only on form
                   has a priori no way to learn meaning. In keeping with
                   the ACL 2020 theme of "Taking Stock of Where We've
                   Been and Where We're Going", we argue that a clear
                   understanding of the distinction between form and
                   meaning will help guide the field towards better
                   science around natural language understanding.},
  url =           {https://openreview.net/forum?id=GKTvAcb12b},
}

